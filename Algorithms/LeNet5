# This code defines the LeNet-5 architecture using PyTorch's nn.Module. It consists of two convolutional layers followed by three fully connected layers. The ReLU activation function is used after each convolutional and fully connected layer, except for the output layer. Max pooling is applied after each convolutional layer to downsample the feature maps. The input images are expected to be grayscale images with dimensions 32x32 pixels.

import torch
import torch.nn as nn
import torch.nn.functional as F

class LeNet5(nn.Module):
    def __init__(self):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)  # 1 input channel, 6 output channels, 5x5 kernel
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # 6 input channels, 16 output channels, 5x5 kernel
        self.fc1 = nn.Linear(16 * 5 * 5, 120)        # 16 * 5 * 5 input features, 120 output features
        self.fc2 = nn.Linear(120, 84)                # 120 input features, 84 output features
        self.fc3 = nn.Linear(84, 10)                 # 84 input features, 10 output features (classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))          # Apply convolution and ReLU activation
        x = F.max_pool2d(x, 2)             # Apply max pooling
        x = F.relu(self.conv2(x))          # Apply convolution and ReLU activation
        x = F.max_pool2d(x, 2)             # Apply max pooling
        x = x.view(-1, 16 * 5 * 5)         # Reshape the tensor for fully connected layers
        x = F.relu(self.fc1(x))            # Apply fully connected layer and ReLU activation
        x = F.relu(self.fc2(x))            # Apply fully connected layer and ReLU activation
        x = self.fc3(x)                    # Apply final fully connected layer (no activation)
        return x

# Create an instance of the LeNet-5 model
model = LeNet5()

# Print the model architecture
print(model)
